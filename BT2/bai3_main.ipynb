{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28aacfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2 \n",
    "\n",
    "def collate_fn(samples: list[dict]) -> dict:\n",
    "    #images = [sample['image'].permute(1, 2, -1).unsqueeze(0) for sample in samples] \n",
    "    images = [sample['image'] for sample in samples]\n",
    "    labels = [sample['label'] for sample in samples] \n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "class VinaFood(Dataset):\n",
    "    #def __init__(self, path: str):\n",
    "    def __init__(self, path: str, image_size: tuple[int]):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.image_size = image_size\n",
    "        self.label2idx = {}\n",
    "        self.idx2label = {}\n",
    "        self.data: list[dict] = self.load_data(path)\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        data = []\n",
    "        label_id = 0\n",
    "        print(f\"Loading data from: {path}\")\n",
    "        for folder in os.listdir(path):\n",
    "            label = folder\n",
    "            if label not in self.label2idx:\n",
    "                self.label2idx[label] = label_id\n",
    "                label_id += 1\n",
    "            folder_path = os.path.join(path, folder)\n",
    "            print(f\"Processing folder: {folder} (label_id: {self.label2idx[label]})\")\n",
    "            \n",
    "            for image_file in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                data.append({\n",
    "                    'image': image,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "        self.idx2label = {id: label for label, id in self.label2idx.items()}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        # image = cv2.resize(image, (224, 224))\n",
    "        # label_id = self.label2idx[label]\n",
    "        \n",
    "        image = cv2.resize(image, self.image_size)\n",
    "        # Convert to RGB if needed (OpenCV loads in BGR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Convert to tensor once\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2,0,1) / 255.0\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': self.label2idx[label]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc23b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, image_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.c, self.h, self.w = image_size\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.c,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "        # Define ResNet Blocks\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResNetBlock(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            ResNetBlock(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResNetBlock(64, 128, kernel_size=3, stride=2, padding=1, conv=True, padding_identity=0, stride_identity=2),\n",
    "            ResNetBlock(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            ResNetBlock(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResNetBlock(128, 256, kernel_size=3, stride=2, padding=1, conv=True, padding_identity=0, stride_identity=2),\n",
    "            ResNetBlock(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResNetBlock(256, 512, kernel_size=3, stride=2, padding=1, conv=True, padding_identity=0, stride_identity=2),\n",
    "            ResNetBlock(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: Tensor(B, C, H, W)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B, 64, H/2, W/2)\n",
    "        x = self.maxpool(x)                   # (B, 64, H/4, W/4)\n",
    "        x = self.layer1(x)                    # (B, 64, H/4, W/4)\n",
    "        x = self.layer2(x)                    # (B, 128, H/8, W/8)\n",
    "        x = self.layer3(x)                    # (B, 256, H/16, W/16)\n",
    "        x = self.layer4(x)                    # (B, 512, H/32, W/32)\n",
    "        x = self.avgpool(x)                   # (B, 512, 1, 1)\n",
    "        x = torch.flatten(x, 1)               # (B, 512)\n",
    "        x = self.fc(x)                        # (B, num_labels)\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, conv=False, padding_identity=0, stride_identity=1):\n",
    "        super().__init__()\n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Second convolution layer (stride should be 1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,  # Always 1 for conv2\n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.use_shortcut = conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=stride_identity,\n",
    "                padding=padding_identity\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ) if conv else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        if self.use_shortcut:\n",
    "            identity = self.shortcut(x)\n",
    "            \n",
    "        # Add shortcut\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed5ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: D:\\NguyenTienDat_23520262\\Nam_3\\DL\\BT2\\VinaFood21\\train\n",
      "Processing folder: banh-can (label_id: 0)\n",
      "Processing folder: banh-hoi (label_id: 1)\n",
      "Processing folder: banh-mi-chao (label_id: 2)\n",
      "Processing folder: banh-tet (label_id: 3)\n",
      "Processing folder: banh-trang-tron (label_id: 4)\n",
      "Processing folder: banh-u (label_id: 5)\n",
      "Processing folder: banh-uot (label_id: 6)\n",
      "Processing folder: bap-nuong (label_id: 7)\n",
      "Processing folder: bo-kho (label_id: 8)\n",
      "Processing folder: bo-la-lot (label_id: 9)\n",
      "Processing folder: bot-chien (label_id: 10)\n",
      "Processing folder: ca-ri (label_id: 11)\n",
      "Processing folder: canh-kho-qua (label_id: 12)\n",
      "Processing folder: canh-khoai-mo (label_id: 13)\n",
      "Processing folder: ga-nuong (label_id: 14)\n",
      "Processing folder: goi-ga (label_id: 15)\n",
      "Processing folder: ha-cao (label_id: 16)\n",
      "Processing folder: hoanh-thanh-nuoc (label_id: 17)\n",
      "Processing folder: pha-lau (label_id: 18)\n",
      "Processing folder: tau-hu (label_id: 19)\n",
      "Processing folder: thit-kho-trung (label_id: 20)\n",
      "Epoch: 1\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 8 11 18 18 16  3  6  9  5  4 18  9  9 17 15 10 20 12 11  1  4  9 20  4\n",
      " 18  4 14  6 10  1  8  6]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[ 0.06088367 -0.24695812  0.28413075 -0.7402329  -0.50142294 -0.41597056\n",
      " -0.12530538  0.41606426 -0.7221879   0.3982368   0.536271   -0.12814468\n",
      "  0.6379715   1.6739011  -1.3655033  -0.26727608  0.28638265 -1.0905709\n",
      " -0.82756186  1.6798155   0.0070526 ]\n",
      "\n",
      "Loss: 2.6664681028408608\n",
      "Unique predictions: [ 1  2  3  4  6  8  9 10 11 12 13 14 15 16 18 19]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.20730418817077784\n",
      "precision: 0.2252220726296547\n",
      "f1: 0.17544340850241444\n",
      "Epoch: 2\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [20 14 14 18 17 16 12  8  6 14  2 16 16 12  0 19 13 10 11  1 20  7  6  2\n",
      "  6 20 15  8  8 17  6 14]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-1.233499    0.26373878  2.0145514  -3.051372    1.1684186  -2.3388\n",
      "  0.49706793 -0.46864948  2.594655   -0.4665273   0.7903861   0.8132232\n",
      " -4.202163   -3.491277    3.3277903  -0.06304626 -0.90127295 -2.7346628\n",
      "  2.1571724  -3.9174945   1.1592586 ]\n",
      "\n",
      "Loss: 2.3443311437679704\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.27492736268034196\n",
      "precision: 0.34220360728533794\n",
      "f1: 0.24765744018555905\n",
      "Epoch: 3\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [13  2  4 17  6  9 14  6 13 17  2 10  9  1 11  4  6 10 16 14 14  6 16  6\n",
      " 18 11 10  6  2  0 15  2]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-0.9471353   0.5469965  -1.6974981   0.48626715 -0.78714263  0.4137811\n",
      " -0.3551232  -0.9308335   0.37689695 -0.18290773 -0.99545443 -1.0447742\n",
      " -0.5911772   0.96058285 -0.43922693 -1.74429     0.92191446 -0.83399004\n",
      "  0.19099101  0.13498926  0.69441545]\n",
      "\n",
      "Loss: 2.202193423820909\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.3247479864980882\n",
      "precision: 0.4126784644467931\n",
      "f1: 0.30962893848665834\n",
      "Epoch: 4\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 6  9 19 16 16  6 18 16  1 18  6  3 18 15 17 14  8  0 12 16  2 11  6  9\n",
      "  8 16  7  5 14 18  4  4]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[ 1.4157572   3.8381612   1.8471472  -2.2076602   0.66500103 -4.875475\n",
      "  4.3071003  -1.5206907  -3.7875137  -4.455895    0.52357715 -5.149605\n",
      " -3.8217378  -4.4616556   0.41534042  2.6923013   3.5657763  -0.45607772\n",
      " -5.673278   -2.6277506   0.11381502]\n",
      "\n",
      "Loss: 2.0511979866938987\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.2819391567681464\n",
      "precision: 0.3769984912005097\n",
      "f1: 0.2469198150064415\n",
      "Epoch: 5\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 8  9  4  3 16 11  1  1  8  6 12  2  6  8  1  6 15 16 17 18  2 12  1 18\n",
      " 10 20  8  5  8  9 20  6]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-0.34931758  0.12870783  0.18881533 -2.834208    0.94345534 -2.262703\n",
      "  0.31250122 -1.2463884   1.5459502  -1.1019924  -0.16638988 -0.20399754\n",
      " -2.2096426  -2.087703   -0.19488938 -0.7491749  -0.8820332  -0.9036255\n",
      "  1.0171332  -2.6709666   0.1750552 ]\n",
      "\n",
      "Loss: 1.9394466820036529\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.2404283511112546\n",
      "precision: 0.4850050956916659\n",
      "f1: 0.22736918393537464\n",
      "Epoch: 6\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 5  7  1  1  7  1 18 16 18 12  0 20 14 17  1 18  1 19  9 14 14 17  1  5\n",
      "  0 14  3  3  9  1  6 19]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-1.891248   -0.8138542   0.6792591  -0.74509025 -0.85818696 -0.18965596\n",
      " -1.1856613  -1.8564945   0.7952893  -1.1818806  -0.92181456 -0.06388611\n",
      "  1.4988515  -1.1085684  -0.04714739 -0.90415627 -1.0232482   0.48248467\n",
      "  1.3925211  -4.3595      1.1123207 ]\n",
      "\n",
      "Loss: 1.819055534092484\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.3624672430959012\n",
      "precision: 0.5023998206225463\n",
      "f1: 0.35774281451836226\n",
      "Epoch: 7\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [10 16 14 11  8  9 20  1 18  6 10 19  8 18 15 16  1  2 20  2  9 19 18  5\n",
      " 19  4 17 10 14 14  4 18]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-1.8908237  -0.3772038  -3.0884852  -7.352363    2.142873   -2.350827\n",
      "  1.1782984   1.4754343   1.3513267  -0.5562252   2.573019    0.36439013\n",
      " -3.0622916  -2.6040463  -0.29989302  0.35261434 -1.7862494  -1.733978\n",
      "  0.24987185 -3.4142451  -1.3056939 ]\n",
      "\n",
      "Loss: 1.7283558522819713\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.3042120477961773\n",
      "precision: 0.40338174107023195\n",
      "f1: 0.2740577052387841\n",
      "Epoch: 8\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 6 18 11  8  9 12  6  6  1  5  5 18  5  5  1 20 18 18  1  1  5  3  0  6\n",
      "  1 16 16 10 10 12  1  5]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[-1.3981054  -0.43570262 -0.1899276  -0.43081343  0.7936604  -1.5939451\n",
      "  0.22984424 -1.3903754  -0.43118316 -2.094518    0.4766504   0.2015611\n",
      "  0.9734094   1.0391276  -1.1867771  -1.2157416  -1.2330456   0.48647705\n",
      "  0.4430434  -3.2826462  -0.5343553 ]\n",
      "\n",
      "Loss: 1.6139596594367058\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.4598610314378656\n",
      "precision: 0.5079743571968097\n",
      "f1: 0.4320633266206202\n",
      "Epoch: 9\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [ 6  3 14  6  8  3 16 10 17 20  0  0 14  4 16 12 14 13  4  4  9  2 10 16\n",
      "  3  6 20  1  4  8  6 10]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[ 0.40536252  1.467954   -1.2548081  -5.514317    0.5461683  -4.077153\n",
      "  3.150318   -2.6943107  -2.3519475  -3.8670425  -0.981596   -0.9160847\n",
      " -5.278683   -4.851475    0.31521404 -0.39999607  2.1198375  -2.6089046\n",
      " -2.8327892  -1.6133714   0.425111  ]\n",
      "\n",
      "Loss: 1.5121699133116728\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.4487802093046224\n",
      "precision: 0.589276277344001\n",
      "f1: 0.4677841426986167\n",
      "Epoch: 10\n",
      "\n",
      "Image shape: torch.Size([32, 3, 224, 224])\n",
      "Label shape: torch.Size([32])\n",
      "Label values: [19  9 10 15 19 18  2 17  9 20  1  6 10  2 15 19  4 14  1 11  1  6  9  7\n",
      " 14 14  1  3  9 12 16 10]\n",
      "Output shape: torch.Size([32, 21])\n",
      "Output sample: \n",
      "[ 0.0531483  -0.7259956   2.003098    0.67095214 -1.7223561  -0.3593541\n",
      " -2.3272996   0.81265885 -3.7579813  -1.0533603  -2.8759696  -2.8949783\n",
      " -4.0436697  -2.9709797  -1.6522474  -4.4076085   2.4456685  -4.6406627\n",
      " -1.6235774   1.8090878   0.7516042 ]\n",
      "\n",
      "Loss: 1.3773615529203111\n",
      "Unique predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Unique true labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "recall: 0.43487552189218187\n",
      "precision: 0.6190866179588709\n",
      "f1: 0.43573754003469295\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn \n",
    "import torch\n",
    "import numpy as np \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from vinafood_dataset import VinaFood, collate_fn\n",
    "from ResNet_18 import ResNet18\n",
    "\n",
    "device = \"cpu\"\n",
    "image_size = (224, 224)\n",
    "\n",
    "train_dataset = VinaFood(\n",
    "    path=r\"D:\\NguyenTienDat_23520262\\Nam_3\\DL\\BT2\\VinaFood21\\train\",\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "# test_dataset = VinaFood(\n",
    "#     path=r\"D:\\NguyenTienDat_23520262\\Nam_3\\DL\\BT2\\VinaFood21\\test\",\n",
    "#     image_size=image_size\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=32,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "image_size = (3, ) + image_size   # (3, 224, 224)\n",
    "model = ResNet18(num_labels=len(train_dataset.idx2label), image_size=image_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval() \n",
    "    outputs = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            image = item[\"image\"].to(device) \n",
    "            label = item[\"label\"].to(device) \n",
    "            output = model(image)   \n",
    "            predictions = torch.argmax(output, dim=-1)  \n",
    "\n",
    "            outputs.extend(predictions.cpu().numpy())\n",
    "            trues.extend(label.cpu().numpy())\n",
    "    \n",
    "    # Print unique values for debugging\n",
    "    print(f\"Unique predictions: {np.unique(outputs)}\")\n",
    "    print(f\"Unique true labels: {np.unique(trues)}\")\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            \"recall\": recall_score(trues, outputs, average=\"macro\", zero_division=0),\n",
    "            \"precision\": precision_score(trues, outputs, average=\"macro\", zero_division=0),\n",
    "            \"f1\": f1_score(trues, outputs, average=\"macro\", zero_division=0),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metrics calculation: {e}\")\n",
    "        return {\n",
    "            \"recall\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"f1\": 0.0\n",
    "        }\n",
    "\n",
    "EPOCHS = 10 \n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "    losses = []\n",
    "    model.train() \n",
    "    for batch_idx, item in enumerate(train_loader):\n",
    "        image = item[\"image\"].to(device)\n",
    "        label = item[\"label\"].to(device)\n",
    "        \n",
    "        # Print shapes and values for first batch of each epoch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"\\nImage shape: {image.shape}\")\n",
    "            print(f\"Label shape: {label.shape}\")\n",
    "            print(f\"Label values: {label.cpu().numpy()}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image)\n",
    "        \n",
    "        # Print output info for first batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            print(f\"Output sample: \\n{output[0].cpu().detach().numpy()}\\n\")\n",
    "        \n",
    "        loss = loss_fn(output, label.long())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss: {(np.array(losses).mean())}\")\n",
    "    metrics = evaluate(model, train_loader)\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric}: {metrics[metric]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
